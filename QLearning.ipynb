{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1abb9ef5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update counts:\n",
      "---------------------------\n",
      " 0.27| 0.05| 0.04| 0.00|\n",
      "---------------------------\n",
      " 0.12| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.28| 0.07| 0.05| 0.11|\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "from td0_prediction import random_action\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "  # suboptimal policies\n",
    "  # e.g.\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   R* |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   R  |   U  |   L  |\n",
    "  # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "  # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "  # point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "  # on right.\n",
    "  # instead, let's penalize each movement so the agent will find a shorter route.\n",
    "  #\n",
    "  # grid = standard_grid()\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # no policy initialization, we will derive our policy from most recent Q\n",
    "\n",
    "  # initialize Q(s,a)\n",
    "  Q = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      Q[s][a] = 0\n",
    "\n",
    "  # let's also keep track of how many times Q[s] has been updated\n",
    "  update_counts = {}\n",
    "  update_counts_sa = {}\n",
    "  for s in states:\n",
    "    update_counts_sa[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      update_counts_sa[s][a] = 1.0\n",
    "\n",
    "  # repeat until convergence\n",
    "  t = 1.0\n",
    "  deltas = []\n",
    "  for it in range(10000):\n",
    "    if it % 100 == 0:\n",
    "      t += 1e-2\n",
    "    if it % 2000 == 0:\n",
    "      print(\"it:\", it)\n",
    "\n",
    "    # instead of 'generating' an epsiode, we will PLAY\n",
    "    # an episode within this loop\n",
    "    s = (2, 0) # start state\n",
    "    grid.set_state(s)\n",
    "\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    a, _ = max_dict(Q[s])\n",
    "    biggest_change = 0\n",
    "    while not grid.game_over():\n",
    "      a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "      # random action also works, but slower since you can bump into walls\n",
    "      # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "      r = grid.move(a)\n",
    "      s2 = grid.current_state()\n",
    "\n",
    "      # adaptive learning rate\n",
    "      alpha = ALPHA / update_counts_sa[s][a]\n",
    "      update_counts_sa[s][a] += 0.005\n",
    "\n",
    "      # we will update Q(s,a) AS we experience the episode\n",
    "      old_qsa = Q[s][a]\n",
    "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "      # we will use this max[a']{ Q(s',a')} in our update\n",
    "      # even if we do not end up taking this action in the next step\n",
    "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "      # we would like to know how often Q(s) has been updated too\n",
    "      update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "      # next state becomes current state\n",
    "      s = s2\n",
    "     \n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # determine the policy from Q*\n",
    "  # find V* from Q*\n",
    "  policy = {}\n",
    "  V = {}\n",
    "  for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "  # what's the proportion of time we spend updating each part of Q?\n",
    "  print(\"update counts:\")\n",
    "  total = np.sum(list(update_counts.values()))\n",
    "  for k, v in update_counts.items():\n",
    "    update_counts[k] = float(v) / total\n",
    "  print_values(update_counts, grid)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we remove the epsilon greedy policy and use any possible action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEstJREFUeJzt3X+QXWV9x/H3lw0BQTQoOxqT0AQnFXc6o6Q7NIhVC7Yl\n1JqZTseGGYrS1pQp1F91bKi12uk/bYdxKFNKzCBOqRZUZGoGo1j81XYqyCI2EkJkDUqShmahCkoq\nSci3f9wTvFn35J7d7ObePOf9mtnZe57nOfd+nw189uxzzr0nMhNJUnuc0O8CJEnHlsEvSS1j8EtS\nyxj8ktQyBr8ktYzBL0ktY/BLUssY/JLUMga/JLXMvH4XMJUzzjgjly5d2u8yJOm4cd999z2emcNN\nxg5k8C9dupSxsbF+lyFJx42I+H7TsS71SFLLGPyS1DIGvyS1jMEvSS1j8EtSyzQK/oi4KCK2RcR4\nRKybov/siPh6RDwTEe+dzr6SpGOrZ/BHxBBwPbAKGAEuiYiRScP+F3gHcM0M9pUkHUNNjvjPBcYz\nc3tm7gNuBVZ3D8jMPZl5L7B/uvvOpld+4Av86W2b5+rpJakITYJ/EbCja3tn1dZE430jYm1EjEXE\n2MTERMOnP9z/7X+WT47t6D1QklpsYE7uZuaGzBzNzNHh4UbvOpYkzUCT4N8FLOnaXly1NXE0+0qS\n5kCT4L8XWB4RyyJiPrAG2Njw+Y9mX0nSHOj5IW2ZeSAirgLuBIaAmzJzS0RcUfWvj4iXAmPAC4CD\nEfEuYCQzn5pq37majCSpt0afzpmZm4BNk9rWdz1+jM4yTqN9JUn9MzAndyVJx4bBL0ktY/BLUssY\n/JLUMga/JLWMwS9JLWPwS1LLGPyS1DIGvyS1jMEvSS1j8EtSyxj8ktQyBr8ktYzBL0ktY/BLUssY\n/JLUMga/JLWMwS9JLWPwS1LLGPyS1DIGvyS1jMEvSS1j8EtSyxj8ktQyBr8ktYzBL0ktY/BLUssY\n/JLUMga/JLWMwS9JLdMo+CPioojYFhHjEbFuiv6IiOuq/s0RsaKr790RsSUiHoiIWyLi5NmcgCRp\nenoGf0QMAdcDq4AR4JKIGJk0bBWwvPpaC9xQ7bsIeAcwmpm/AAwBa2ateknStDU54j8XGM/M7Zm5\nD7gVWD1pzGrg5uy4G1gQEQurvnnA8yJiHnAK8N+zVLskaQaaBP8iYEfX9s6qreeYzNwFXAM8CuwG\nnszML868XEnS0ZrTk7sRcTqdvwaWAS8DTo2IS2vGro2IsYgYm5iYOKrXzcyj2l+SStYk+HcBS7q2\nF1dtTca8EXgkMycycz9wO/CaqV4kMzdk5mhmjg4PDzetf0rmviTVaxL89wLLI2JZRMync3J246Qx\nG4HLqqt7VtJZ0tlNZ4lnZUScEhEBXAhsncX6p2TuS1K9eb0GZOaBiLgKuJPOVTk3ZeaWiLii6l8P\nbAIuBsaBvcDlVd89EXEb8E3gAHA/sGEuJiJJaqZn8ANk5iY64d7dtr7rcQJX1uz7QeCDR1HjtHXK\niWP5kpJ03Cjynbsu9UhSvTKD3+SXpFplBr/H/JJUq8jglyTVKzL4XeqRpHpFBr8kqV6Rwe8RvyTV\nKzP4PbkrSbXKDH5zX5JqFRn8kqR6RQa/B/ySVK/M4HetR5JqlRn8/S5AkgZYmcFv8ktSrSKDX5JU\nr8zg94hfkmoVGfy+gUuS6pUZ/Oa+JNUqM/j7XYAkDbAyg99DfkmqVWTwS5LqFRn8Hu9LUr0yg9/k\nl6RaZQa/x/ySVKvI4Df3JalekcFv7ktSvSKDX5JUr8jg9+SuJNUrM/hd7JGkWmUGv7kvSbXKDP5+\nFyBJA6xR8EfERRGxLSLGI2LdFP0REddV/ZsjYkVX34KIuC0iHoqIrRFx3mxOQJI0PT2DPyKGgOuB\nVcAIcElEjEwatgpYXn2tBW7o6vs74AuZeTbwKmDrLNR9RH5ImyTVa3LEfy4wnpnbM3MfcCuwetKY\n1cDN2XE3sCAiFkbEC4HXAR8FyMx9mfnDWax/Sua+JNVrEvyLgB1d2zurtiZjlgETwMci4v6IuDEi\nTp3qRSJibUSMRcTYxMRE4wlIkqZnrk/uzgNWADdk5jnA08DPnCMAyMwNmTmamaPDw8NH9aIe8UtS\nvSbBvwtY0rW9uGprMmYnsDMz76nab6Pzi2BOeR2/JNVrEvz3AssjYllEzAfWABsnjdkIXFZd3bMS\neDIzd2fmY8COiHhFNe5C4MHZKl6SNH3zeg3IzAMRcRVwJzAE3JSZWyLiiqp/PbAJuBgYB/YCl3c9\nxR8Dn6h+aWyf1DcnXOqRpHo9gx8gMzfRCffutvVdjxO4smbfbwGjR1HjtJn7klSvzHfuesgvSbXK\nDP5+FyBJA6zI4Jck1Ssy+F3pkaR6RQa/iz2SVK/I4PeIX5LqlRn8/S5AkgZYmcFv8ktSrSKDX5JU\nr8jg/8Heff0uQZIGVpHB/6GNW/pdgiQNrCKD/5kDB/tdgiQNrCKD/6BndyWpVpHBb+5LUr0yg98r\n+SWpVpnBb+5LUi2DX5Japsjg9+SuJNUrMvjNfUmqV2bwe3JXkmqVGfzmviTVKir4V7/6ZQAcNPgl\nqVZRwf/8k+YBkB7yS1KtooI/ovPd2JekekUF/wlV8nvEL0n1igr+6oDfNX5JOoKygt8jfknqqajg\nP8TYl6R6RQX/CZ7dlaSeCgv+zvdnXeqRpFqNgj8iLoqIbRExHhHrpuiPiLiu6t8cESsm9Q9FxP0R\nccdsFT51nZ3v5r4k1esZ/BExBFwPrAJGgEsiYmTSsFXA8uprLXDDpP53AluPutoenruc07UeSarV\n5Ij/XGA8M7dn5j7gVmD1pDGrgZuz425gQUQsBIiIxcBvADfOYt1Tq474vZxTkuo1Cf5FwI6u7Z1V\nW9Mx1wLvAw7OsMbGAk/uSlIvc3pyNyLeBOzJzPsajF0bEWMRMTYxMTGj1zvhudw3+SWpTpPg3wUs\n6dpeXLU1GXM+8OaI+B6dJaILIuLjU71IZm7IzNHMHB0eHm5Y/uEOrfG71CNJ9ZoE/73A8ohYFhHz\ngTXAxkljNgKXVVf3rASezMzdmXl1Zi7OzKXVfl/OzEtncwLd4rk1fpNfkurM6zUgMw9ExFXAncAQ\ncFNmbomIK6r+9cAm4GJgHNgLXD53Jdc79Fk95r4k1esZ/ACZuYlOuHe3re96nMCVPZ7jq8BXp13h\nNBz6rB5JUr3C3rlr8EtSL0UFv7kvSb2VFfz9LkCSjgNFBf8JJxj9ktRLUcHvUo8k9VZW8LvYI0k9\nFRX8L33hSf0uQZIGXlHBf8HZL+l3CZI08IoKftf4Jam3ooJfktRbUcHvAb8k9VZW8LvWI0k9FRX8\nkqTeigp+j/clqbeigr/bj5850O8SJGkgFRX83Uv8jz6xt3+FSNIAKyr4JUm9FRX8flaPJPVWVPBL\nknoz+CWpZYoKft+/JUm9FRX83bycU5KmVmzwv+UjX+93CZI0kIoNfknS1IoK/iFvti5JPRUV/CcO\nFTUdSZoTJqUktYzBL0ktY/BLUssUHfyPPP50v0uQpIFTdPD/yjVf7XcJkjRwGgV/RFwUEdsiYjwi\n1k3RHxFxXdW/OSJWVO1LIuIrEfFgRGyJiHfO9gQkSdPTM/gjYgi4HlgFjACXRMTIpGGrgOXV11rg\nhqr9APAnmTkCrASunGJfSdIx1OSI/1xgPDO3Z+Y+4FZg9aQxq4Gbs+NuYEFELMzM3Zn5TYDM/BGw\nFVg0i/VLkqapSfAvAnZ0be/kZ8O755iIWAqcA9wz1YtExNqIGIuIsYmJiQZlSZJm4pic3I2I5wOf\nAd6VmU9NNSYzN2TmaGaODg8PH4uyJKmVmgT/LmBJ1/biqq3RmIg4kU7ofyIzb595qZKk2dAk+O8F\nlkfEsoiYD6wBNk4asxG4rLq6ZyXwZGbujogAPgpszcwPz2rlkqQZmddrQGYeiIirgDuBIeCmzNwS\nEVdU/euBTcDFwDiwF7i82v184HeBb0fEt6q2P8vMTbM7DUlSUz2DH6AK6k2T2tZ3PU7gyin2+w/A\nz0qWpAFS9Dt3AX6y/9l+lyBJA6X44H/ae+9K0mGKD35J0uGKD/7sdwGSNGCKC/7TTjr8fHWa/JJ0\nmOKC/23nL+13CZI00IoLfknSkRUX/Oe9/MWHbaer/JJ0mOKC/7STTjxs+2vb/KRPSepWXPDPn3f4\nlP768w/1qRJJGkzFBf8rXnraYdtPPL2vT5VI0mAqLvinsv/Zg/0uQZIGRiuC/y0f+Xq/S5CkgdGK\n4L//0R/2uwRJGhitCH5J0k+1Jvhv/Pft/S5BkgZCa4L/mi9u63cJkjQQWhP8P9nvlT2SBC0KfklS\nh8EvSS3TquBPP5xfktoV/O/99OZ+lyBJfdeq4P/ad/b0uwRJ6rsig//tv7xsyvZ9B7yyR5KKDP46\nT/3kQL9LkKS+KzL4Tzv5xNq+bz76g2NYiSQNniKD/w9ffxZ/8aaRKft+6x/+k4MHvbpHUnsVGfwn\nzRvi9167jFctWTBl/54fPXOMK5KkwVFk8B/yod+c+qj/dzb4+fyS2qvo4D/nzNOnbP/+E3u9wkdS\nazUK/oi4KCK2RcR4RKyboj8i4rqqf3NErGi6b7/8/J9/nvE9P+p3GZJ0zPUM/ogYAq4HVgEjwCUR\nMXkNZRWwvPpaC9wwjX375o0f/jeWrvtcv8uQpGOqyRH/ucB4Zm7PzH3ArcDqSWNWAzdnx93AgohY\n2HDfOfWV976h55il6z7HtXd9Z+6LkaQBMK/BmEXAjq7tncAvNRizqOG+c2rZGafy0F9dxNkf+MIR\nx11718Nce9fDAJx1xqlETP+1YiY7ATPbS1JpTj9lPp+64rw5f50mwX9MRMRaOstEnHnmmbP63Cef\nOMQtb1/JBz77AI//+Bl+uHf/Ece/cuELpp/GM3xrQM50R0nFecER3nw6m5oE/y5gSdf24qqtyZgT\nG+wLQGZuADYAjI6OznoanvfyF3PXe14/208rScedJmv89wLLI2JZRMwH1gAbJ43ZCFxWXd2zEngy\nM3c33FeSdAz1POLPzAMRcRVwJzAE3JSZWyLiiqp/PbAJuBgYB/YClx9p3zmZiSSpkRjEu1KNjo7m\n2NhYv8uQpONGRNyXmaNNxhb9zl1J0s8y+CWpZQx+SWoZg1+SWsbgl6SWGcireiJiAvj+DHc/A3h8\nFss5Hjjn8rVtvuCcp+vnMnO4ycCBDP6jERFjTS9pKoVzLl/b5gvOeS651CNJLWPwS1LLlBj8G/pd\nQB845/K1bb7gnOdMcWv8kqQjK/GIX5J0BMUE/6De1H0mImJJRHwlIh6MiC0R8c6q/UUR8a8R8XD1\n/fSufa6u5r4tIn69q/0XI+LbVd91MdPbhB0DETEUEfdHxB3VdunzXRARt0XEQxGxNSLOa8Gc3139\nN/1ARNwSESeXNueIuCki9kTEA11tszbHiDgpIj5Ztd8TEUunXWRmHvdfdD7y+bvAWcB84L+AkX7X\ndRTzWQisqB6fBnyHzs3q/xZYV7WvA/6mejxSzfkkYFn1sxiq+r4BrKRzT7HPA6v6Pb8jzPs9wD8D\nd1Tbpc/3H4E/qB7PBxaUPGc6t2J9BHhetf0p4G2lzRl4HbACeKCrbdbmCPwRsL56vAb45LRr7PcP\naZZ+0OcBd3ZtXw1c3e+6ZnF+nwV+FdgGLKzaFgLbppovnfsfnFeNeair/RLgI/2eT80cFwNfAi7o\nCv6S5/vCKgRjUnvJcz50D+4X0bkXyB3Ar5U4Z2DppOCftTkeGlM9nkfnDV8xnfpKWeqpu9n7ca/6\nM+4c4B7gJdm5sxnAY8BLqsdHutn9zinaB9G1wPuAg11tJc93GTABfKxa3roxIk6l4Dln5i7gGuBR\nYDedO/V9kYLn3GU25/jcPpl5AHgSePF0iikl+IsUEc8HPgO8KzOf6u7Lzq/7Ii7Jiog3AXsy8766\nMSXNtzKPznLADZl5DvA0nSWA55Q252pdezWdX3ovA06NiEu7x5Q256kMwhxLCf4mN4Q/rkTEiXRC\n/xOZeXvV/D8RsbDqXwjsqdrr5r+rejy5fdCcD7w5Ir4H3ApcEBEfp9z5QucIbmdm3lNt30bnF0HJ\nc34j8EhmTmTmfuB24DWUPedDZnOOz+0TEfPoLBs+MZ1iSgn+om7qXp29/yiwNTM/3NW1EXhr9fit\ndNb+D7Wvqc72LwOWA9+o/rR8KiJWVs95Wdc+AyMzr87MxZm5lM6/3Zcz81IKnS9AZj4G7IiIV1RN\nFwIPUvCc6SzxrIyIU6paLwS2UvacD5nNOXY/12/T+f9len9B9PskyCyeTLmYztUv3wXe3+96jnIu\nr6Xzp+Bm4FvV18V01vG+BDwM3AW8qGuf91dz30bXFQ7AKPBA1ff3TPMkUB/m/gZ+enK36PkCrwbG\nqn/nfwFOb8Gc/xJ4qKr3n+hczVLUnIFb6JzD2E/nL7vfn805AicDnwbG6Vz5c9Z0a/Sdu5LUMqUs\n9UiSGjL4JallDH5JahmDX5JaxuCXpJYx+CWpZQx+SWoZg1+SWub/AS985tPMUuscAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1abb85ad320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update counts:\n",
      "---------------------------\n",
      " 0.14| 0.09| 0.04| 0.00|\n",
      "---------------------------\n",
      " 0.18| 0.00| 0.04| 0.00|\n",
      "---------------------------\n",
      " 0.23| 0.16| 0.08| 0.04|\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "from td0_prediction import random_action\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "  # suboptimal policies\n",
    "  # e.g.\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   R* |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   R  |   U  |   L  |\n",
    "  # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "  # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "  # point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "  # on right.\n",
    "  # instead, let's penalize each movement so the agent will find a shorter route.\n",
    "  #\n",
    "  # grid = standard_grid()\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # no policy initialization, we will derive our policy from most recent Q\n",
    "\n",
    "  # initialize Q(s,a)\n",
    "  Q = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      Q[s][a] = 0\n",
    "\n",
    "  # let's also keep track of how many times Q[s] has been updated\n",
    "  update_counts = {}\n",
    "  update_counts_sa = {}\n",
    "  for s in states:\n",
    "    update_counts_sa[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      update_counts_sa[s][a] = 1.0\n",
    "\n",
    "  # repeat until convergence\n",
    "  t = 1.0\n",
    "  deltas = []\n",
    "  for it in range(10000):\n",
    "    if it % 100 == 0:\n",
    "      t += 1e-2\n",
    "    if it % 2000 == 0:\n",
    "      print(\"it:\", it)\n",
    "\n",
    "    # instead of 'generating' an epsiode, we will PLAY\n",
    "    # an episode within this loop\n",
    "    s = (2, 0) # start state\n",
    "    grid.set_state(s)\n",
    "\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    a, _ = max_dict(Q[s])\n",
    "    biggest_change = 0\n",
    "    while not grid.game_over():\n",
    "      # a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "      # random action also works, but slower since you can bump into walls\n",
    "      a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "      r = grid.move(a)\n",
    "      s2 = grid.current_state()\n",
    "\n",
    "      # adaptive learning rate\n",
    "      alpha = ALPHA / update_counts_sa[s][a]\n",
    "      update_counts_sa[s][a] += 0.005\n",
    "\n",
    "      # we will update Q(s,a) AS we experience the episode\n",
    "      old_qsa = Q[s][a]\n",
    "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "      # we will use this max[a']{ Q(s',a')} in our update\n",
    "      # even if we do not end up taking this action in the next step\n",
    "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "      # we would like to know how often Q(s) has been updated too\n",
    "      update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "      # next state becomes current state\n",
    "      s = s2\n",
    "     \n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # determine the policy from Q*\n",
    "  # find V* from Q*\n",
    "  policy = {}\n",
    "  V = {}\n",
    "  for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "  # what's the proportion of time we spend updating each part of Q?\n",
    "  print(\"update counts:\")\n",
    "  total = np.sum(list(update_counts.values()))\n",
    "  for k, v in update_counts.items():\n",
    "    update_counts[k] = float(v) / total\n",
    "  print_values(update_counts, grid)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "from td0_prediction import random_action\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "  # suboptimal policies\n",
    "  # e.g.\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   R* |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   R  |   U  |   L  |\n",
    "  # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "  # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "  # point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "  # on right.\n",
    "  # instead, let's penalize each movement so the agent will find a shorter route.\n",
    "  #\n",
    "  # grid = standard_grid()\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # no policy initialization, we will derive our policy from most recent Q\n",
    "\n",
    "  # initialize Q(s,a)\n",
    "  Q = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      Q[s][a] = 0\n",
    "\n",
    "  # let's also keep track of how many times Q[s] has been updated\n",
    "  update_counts = {}\n",
    "  update_counts_sa = {}\n",
    "  for s in states:\n",
    "    update_counts_sa[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      update_counts_sa[s][a] = 1.0\n",
    "\n",
    "  # repeat until convergence\n",
    "  t = 1.0\n",
    "  deltas = []\n",
    "  for it in range(10000):\n",
    "    if it % 100 == 0:\n",
    "      t += 1e-2\n",
    "    if it % 2000 == 0:\n",
    "      print(\"it:\", it)\n",
    "\n",
    "    # instead of 'generating' an epsiode, we will PLAY\n",
    "    # an episode within this loop\n",
    "    s = (2, 0) # start state\n",
    "    grid.set_state(s)\n",
    "\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    a, _ = max_dict(Q[s])\n",
    "    biggest_change = 0\n",
    "    while not grid.game_over():\n",
    "      # a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "      # random action also works, but slower since you can bump into walls\n",
    "      a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "      r = grid.move(a)\n",
    "      s2 = grid.current_state()\n",
    "\n",
    "      # adaptive learning rate\n",
    "      alpha = ALPHA / update_counts_sa[s][a]\n",
    "      update_counts_sa[s][a] += 0.005\n",
    "\n",
    "      # we will update Q(s,a) AS we experience the episode\n",
    "      old_qsa = Q[s][a]\n",
    "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "      # we will use this max[a']{ Q(s',a')} in our update\n",
    "      # even if we do not end up taking this action in the next step\n",
    "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "      # we would like to know how often Q(s) has been updated too\n",
    "      update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "      # next state becomes current state\n",
    "      s = s2\n",
    "     \n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # determine the policy from Q*\n",
    "  # find V* from Q*\n",
    "  policy = {}\n",
    "  V = {}\n",
    "  for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "  # what's the proportion of time we spend updating each part of Q?\n",
    "  print(\"update counts:\")\n",
    "  total = np.sum(list(update_counts.values()))\n",
    "  for k, v in update_counts.items():\n",
    "    update_counts[k] = float(v) / total\n",
    "  print_values(update_counts, grid)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
